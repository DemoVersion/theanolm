/l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab
THEANO_FLAGS=floatX=float32,device=gpu,nvcc.fastmath=True
Reading vocabulary from /l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab.
Number of words in vocabulary: 10001
Number of word classes: 10001
2016-12-22 13:38:35,783 train: TRAINING OPTIONS
2016-12-22 13:38:35,784 train: validation_frequency: 1
2016-12-22 13:38:35,784 train: stopping_criterion: no-improvement
2016-12-22 13:38:35,784 train: batch_size: 32
2016-12-22 13:38:35,784 train: min_epochs: 1
2016-12-22 13:38:35,784 train: sequence_length: 25
2016-12-22 13:38:35,784 train: max_annealing_count: 0
2016-12-22 13:38:35,784 train: max_epochs: 15
2016-12-22 13:38:35,784 train: patience: 0
2016-12-22 13:38:35,784 train: OPTIMIZATION OPTIONS
2016-12-22 13:38:35,784 train: epsilon: 1e-06
2016-12-22 13:38:35,784 train: gradient_decay_rate: 0.9
2016-12-22 13:38:35,784 train: unk_penalty: None
2016-12-22 13:38:35,784 train: cost_function: cross-entropy
2016-12-22 13:38:35,784 train: sqr_gradient_decay_rate: 0.999
2016-12-22 13:38:35,784 train: momentum: 0.9
2016-12-22 13:38:35,785 train: weights: [ 1.]
2016-12-22 13:38:35,785 train: noise_sharing: None
2016-12-22 13:38:35,785 train: method: adagrad
2016-12-22 13:38:35,785 train: ignore_unk: False
2016-12-22 13:38:35,785 train: learning_rate: 1.0
2016-12-22 13:38:35,785 train: num_noise_samples: 1
2016-12-22 13:38:35,785 train: max_gradient_norm: 5.0
Creating trainer.
Computing unigram probabilities and the number of mini-batches in training data.
2016-12-22 13:38:36,967 __init__: One epoch of training data contains 1778 mini-batch updates.
2016-12-22 13:38:36,967 __init__: Class unigram probabilities are in the range [0.00000103, 0.05232915].
2016-12-22 13:38:36,970 __init__: Finding sentence start positions in /teamwork/t40511_asr/c/penn-treebank-project/ptb.train.txt.
2016-12-22 13:38:37,034 _reset: Generating a random order of input lines.
Building neural network.
2016-12-22 13:38:37,053 __init__: Creating layers.
2016-12-22 13:38:37,053 __init__: - NetworkInput name=word_input inputs=[] size=10001, devices=[]
2016-12-22 13:38:37,053 __init__: - ProjectionLayer name=projection_layer inputs=[word_input] size=100, devices=[None]
2016-12-22 13:38:37,152 add:      * layers/projection_layer/W size=1000100 type=float32 device=None
2016-12-22 13:38:37,152 __init__: - LSTMLayer name=hidden_layer inputs=[projection_layer] size=256, devices=[None]
2016-12-22 13:38:37,160 add:      * layers/hidden_layer/layer_input/W size=102400 type=float32 device=None
2016-12-22 13:38:37,474 add:      * layers/hidden_layer/step_input/W size=262144 type=float32 device=None
2016-12-22 13:38:37,475 add:      * layers/hidden_layer/layer_input/b size=1024 type=float32 device=None
2016-12-22 13:38:37,475 __init__: - SoftmaxLayer name=output_layer inputs=[hidden_layer] size=10001, devices=[None]
2016-12-22 13:38:37,695 add:      * layers/output_layer/input/W size=2560256 type=float32 device=None
2016-12-22 13:38:37,695 add:      * layers/output_layer/input/b size=10001 type=float32 device=None
2016-12-22 13:38:37,695 __init__: Total number of parameters: 3935925
Compiling optimization function.
2016-12-22 13:38:38,374 add:      * layers/hidden_layer/layer_input/b_gradient size=1024 type=float32 device=None
2016-12-22 13:38:38,374 add:      * layers/hidden_layer/layer_input/b_sum_sqr_gradient size=1024 type=float32 device=None
2016-12-22 13:38:38,374 add:      * layers/output_layer/input/b_gradient size=10001 type=float32 device=None
2016-12-22 13:38:38,375 add:      * layers/output_layer/input/b_sum_sqr_gradient size=10001 type=float32 device=None
2016-12-22 13:38:38,377 add:      * layers/projection_layer/W_gradient size=1000100 type=float32 device=None
2016-12-22 13:38:38,379 add:      * layers/projection_layer/W_sum_sqr_gradient size=1000100 type=float32 device=None
2016-12-22 13:38:38,380 add:      * layers/hidden_layer/step_input/W_gradient size=262144 type=float32 device=None
2016-12-22 13:38:38,380 add:      * layers/hidden_layer/step_input/W_sum_sqr_gradient size=262144 type=float32 device=None
2016-12-22 13:38:38,381 add:      * layers/hidden_layer/layer_input/W_gradient size=102400 type=float32 device=None
2016-12-22 13:38:38,381 add:      * layers/hidden_layer/layer_input/W_sum_sqr_gradient size=102400 type=float32 device=None
2016-12-22 13:38:38,386 add:      * layers/output_layer/input/W_gradient size=2560256 type=float32 device=None
2016-12-22 13:38:38,391 add:      * layers/output_layer/input/W_sum_sqr_gradient size=2560256 type=float32 device=None
Building text scorer for cross-validation.
Validation text: /teamwork/t40511_asr/c/penn-treebank-project/ptb.valid.txt
Training neural network.
2016-12-22 13:42:26,292 _log_update: [200] (11.2 %) of epoch 1 -- lr = 1, cost = 5.76, duration = 12.8 ms
2016-12-22 13:42:52,125 _log_update: [400] (22.5 %) of epoch 1 -- lr = 1, cost = 5.67, duration = 12.8 ms
2016-12-22 13:43:17,952 _log_update: [600] (33.7 %) of epoch 1 -- lr = 1, cost = 5.17, duration = 12.8 ms
2016-12-22 13:43:43,795 _log_update: [800] (45.0 %) of epoch 1 -- lr = 1, cost = 5.24, duration = 12.8 ms
2016-12-22 13:44:09,625 _log_update: [1000] (56.2 %) of epoch 1 -- lr = 1, cost = 5.39, duration = 12.8 ms
2016-12-22 13:44:35,469 _log_update: [1200] (67.5 %) of epoch 1 -- lr = 1, cost = 5.13, duration = 12.8 ms
2016-12-22 13:45:01,403 _log_update: [1400] (78.7 %) of epoch 1 -- lr = 1, cost = 5.41, duration = 12.8 ms
2016-12-22 13:45:27,766 _log_update: [1600] (90.0 %) of epoch 1 -- lr = 1, cost = 4.89, duration = 12.8 ms
2016-12-22 13:45:56,851 _validate: [1772] First validation sample, perplexity 147.67.
2016-12-22 13:46:17,433 _validate: [1775] Center of validation, perplexity 147.72.
2016-12-22 13:46:38,246 _validate: [1778] Last validation sample, perplexity 147.49.
2016-12-22 13:46:38,288 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-12-22 13:46:38,288 _log_validation: [1778] Validation set cost history: [147.7]
2016-12-22 13:46:38,289 _reset: Generating a random order of input lines.
Finished training epoch 1 in 0 hours 4.6 minutes. Best validation perplexity 147.67.
2016-12-22 13:46:41,141 _log_update: [22] (1.2 %) of epoch 2 -- lr = 1, cost = 4.90, duration = 12.8 ms
2016-12-22 13:47:07,062 _log_update: [222] (12.5 %) of epoch 2 -- lr = 1, cost = 4.54, duration = 12.8 ms
2016-12-22 13:47:33,000 _log_update: [422] (23.7 %) of epoch 2 -- lr = 1, cost = 4.75, duration = 12.9 ms
2016-12-22 13:47:58,904 _log_update: [622] (35.0 %) of epoch 2 -- lr = 1, cost = 4.93, duration = 12.9 ms
2016-12-22 13:48:24,793 _log_update: [822] (46.2 %) of epoch 2 -- lr = 1, cost = 4.75, duration = 12.8 ms
2016-12-22 13:48:50,662 _log_update: [1022] (57.5 %) of epoch 2 -- lr = 1, cost = 4.80, duration = 12.8 ms
2016-12-22 13:49:16,536 _log_update: [1222] (68.7 %) of epoch 2 -- lr = 1, cost = 4.74, duration = 12.8 ms
2016-12-22 13:49:42,413 _log_update: [1422] (80.0 %) of epoch 2 -- lr = 1, cost = 4.44, duration = 12.8 ms
2016-12-22 13:50:08,289 _log_update: [1622] (91.2 %) of epoch 2 -- lr = 1, cost = 4.87, duration = 12.9 ms
2016-12-22 13:50:34,409 _validate: [1772] First validation sample, perplexity 126.90.
2016-12-22 13:50:54,959 _validate: [1775] Center of validation, perplexity 126.72.
2016-12-22 13:51:15,470 _validate: [1778] Last validation sample, perplexity 126.74.
2016-12-22 13:51:15,494 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-12-22 13:51:15,494 _log_validation: [1778] Validation set cost history: 147.7 [126.7]
2016-12-22 13:51:15,496 _reset: Generating a random order of input lines.
Finished training epoch 2 in 0 hours 4.6 minutes. Best validation perplexity 126.74.
2016-12-22 13:51:21,191 _log_update: [44] (2.5 %) of epoch 3 -- lr = 1, cost = 4.35, duration = 12.9 ms
2016-12-22 13:51:47,055 _log_update: [244] (13.7 %) of epoch 3 -- lr = 1, cost = 4.08, duration = 12.8 ms
2016-12-22 13:52:12,903 _log_update: [444] (25.0 %) of epoch 3 -- lr = 1, cost = 4.73, duration = 12.8 ms
2016-12-22 13:52:38,750 _log_update: [644] (36.2 %) of epoch 3 -- lr = 1, cost = 4.27, duration = 12.8 ms
2016-12-22 13:53:04,625 _log_update: [844] (47.5 %) of epoch 3 -- lr = 1, cost = 4.28, duration = 12.8 ms
2016-12-22 13:53:30,471 _log_update: [1044] (58.7 %) of epoch 3 -- lr = 1, cost = 4.51, duration = 12.8 ms
2016-12-22 13:53:56,310 _log_update: [1244] (70.0 %) of epoch 3 -- lr = 1, cost = 4.13, duration = 12.8 ms
2016-12-22 13:54:22,164 _log_update: [1444] (81.2 %) of epoch 3 -- lr = 1, cost = 4.22, duration = 12.9 ms
2016-12-22 13:54:48,085 _log_update: [1644] (92.5 %) of epoch 3 -- lr = 1, cost = 4.33, duration = 12.8 ms
2016-12-22 13:55:11,352 _validate: [1772] First validation sample, perplexity 124.02.
2016-12-22 13:55:31,902 _validate: [1775] Center of validation, perplexity 124.16.
2016-12-22 13:55:52,420 _validate: [1778] Last validation sample, perplexity 123.95.
2016-12-22 13:55:52,442 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-12-22 13:55:52,442 _log_validation: [1778] Validation set cost history: 147.7 126.7 [124.0]
2016-12-22 13:55:52,444 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 4.6 minutes. Best validation perplexity 124.05.
2016-12-22 13:56:01,030 _log_update: [66] (3.7 %) of epoch 4 -- lr = 1, cost = 4.13, duration = 12.8 ms
2016-12-22 13:56:26,890 _log_update: [266] (15.0 %) of epoch 4 -- lr = 1, cost = 4.44, duration = 12.8 ms
2016-12-22 13:56:52,791 _log_update: [466] (26.2 %) of epoch 4 -- lr = 1, cost = 4.55, duration = 12.8 ms
2016-12-22 13:57:18,649 _log_update: [666] (37.5 %) of epoch 4 -- lr = 1, cost = 4.50, duration = 12.8 ms
2016-12-22 13:57:44,509 _log_update: [866] (48.7 %) of epoch 4 -- lr = 1, cost = 4.38, duration = 12.8 ms
2016-12-22 13:58:10,372 _log_update: [1066] (60.0 %) of epoch 4 -- lr = 1, cost = 4.29, duration = 12.8 ms
2016-12-22 13:58:36,220 _log_update: [1266] (71.2 %) of epoch 4 -- lr = 1, cost = 4.34, duration = 12.8 ms
2016-12-22 13:59:02,079 _log_update: [1466] (82.5 %) of epoch 4 -- lr = 1, cost = 4.35, duration = 12.8 ms
2016-12-22 13:59:27,922 _log_update: [1666] (93.7 %) of epoch 4 -- lr = 1, cost = 4.26, duration = 12.8 ms
2016-12-22 13:59:48,361 _validate: [1772] First validation sample, perplexity 128.81.
2016-12-22 14:00:08,950 _validate: [1775] Center of validation, perplexity 128.56.
2016-12-22 14:00:29,519 _validate: [1778] Last validation sample, perplexity 129.00.
2016-12-22 14:00:29,519 _log_validation: [1778] Validation set cost history: 147.7 126.7 [124.0] 128.8
2016-12-22 14:00:29,520 set_state: layers/projection_layer/W <- array(10001, 100)
2016-12-22 14:00:29,521 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-12-22 14:00:29,521 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-12-22 14:00:29,522 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-12-22 14:00:29,522 set_state: layers/output_layer/input/b <- array(10001,)
2016-12-22 14:00:29,525 set_state: layers/output_layer/input/W <- array(256, 10001)
2016-12-22 14:00:29,527 _reset_state: [1775] (99.83 %) of epoch 3
2016-12-22 14:00:29,527 _log_validation: [1775] Validation set cost history: 147.7 126.7 [124.0]
2016-12-22 14:00:29,527 set_state: Restored iterator to line 42002 of 42068.
2016-12-22 14:00:29,531 set_state: layers/output_layer/input/W_gradient <- array(256, 10001)
2016-12-22 14:00:29,531 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(10001,)
2016-12-22 14:00:29,532 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2016-12-22 14:00:29,532 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2016-12-22 14:00:29,533 set_state: layers/output_layer/input/b_gradient <- array(10001,)
2016-12-22 14:00:29,533 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2016-12-22 14:00:29,534 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2016-12-22 14:00:29,535 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2016-12-22 14:00:29,535 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2016-12-22 14:00:29,538 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 10001)
2016-12-22 14:00:29,539 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
2016-12-22 14:00:29,540 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
Model performance stopped improving. Decreasing learning rate from 1.0 to 0.5 and resetting state to 100 % of epoch 3.
2016-12-22 14:00:29,542 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 4.6 minutes. Best validation perplexity 124.05.
2016-12-22 14:00:40,925 _log_update: [88] (4.9 %) of epoch 4 -- lr = 0.5, cost = 4.20, duration = 12.8 ms
2016-12-22 14:01:06,780 _log_update: [288] (16.2 %) of epoch 4 -- lr = 0.5, cost = 3.66, duration = 12.8 ms
2016-12-22 14:01:32,640 _log_update: [488] (27.4 %) of epoch 4 -- lr = 0.5, cost = 4.05, duration = 12.8 ms
2016-12-22 14:01:58,523 _log_update: [688] (38.7 %) of epoch 4 -- lr = 0.5, cost = 4.13, duration = 12.8 ms
2016-12-22 14:02:24,392 _log_update: [888] (49.9 %) of epoch 4 -- lr = 0.5, cost = 3.88, duration = 12.8 ms
2016-12-22 14:02:50,246 _log_update: [1088] (61.2 %) of epoch 4 -- lr = 0.5, cost = 4.22, duration = 12.8 ms
2016-12-22 14:03:16,201 _log_update: [1288] (72.4 %) of epoch 4 -- lr = 0.5, cost = 4.25, duration = 12.8 ms
2016-12-22 14:03:42,085 _log_update: [1488] (83.7 %) of epoch 4 -- lr = 0.5, cost = 4.28, duration = 12.9 ms
2016-12-22 14:04:07,944 _log_update: [1688] (94.9 %) of epoch 4 -- lr = 0.5, cost = 4.27, duration = 12.8 ms
2016-12-22 14:04:25,535 _validate: [1772] First validation sample, perplexity 126.00.
2016-12-22 14:04:46,123 _validate: [1775] Center of validation, perplexity 126.06.
2016-12-22 14:05:06,691 _validate: [1778] Last validation sample, perplexity 125.92.
2016-12-22 14:05:06,691 _log_validation: [1778] Validation set cost history: 147.7 126.7 [124.0] 126.0
2016-12-22 14:05:06,693 set_state: layers/projection_layer/W <- array(10001, 100)
2016-12-22 14:05:06,693 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-12-22 14:05:06,694 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-12-22 14:05:06,694 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-12-22 14:05:06,695 set_state: layers/output_layer/input/b <- array(10001,)
2016-12-22 14:05:06,698 set_state: layers/output_layer/input/W <- array(256, 10001)
2016-12-22 14:05:06,699 _reset_state: [1775] (99.83 %) of epoch 3
2016-12-22 14:05:06,699 _log_validation: [1775] Validation set cost history: 147.7 126.7 [124.0]
2016-12-22 14:05:06,700 set_state: Restored iterator to line 42002 of 42068.
2016-12-22 14:05:06,703 set_state: layers/output_layer/input/W_gradient <- array(256, 10001)
2016-12-22 14:05:06,704 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(10001,)
2016-12-22 14:05:06,704 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2016-12-22 14:05:06,704 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2016-12-22 14:05:06,705 set_state: layers/output_layer/input/b_gradient <- array(10001,)
2016-12-22 14:05:06,705 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2016-12-22 14:05:06,706 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2016-12-22 14:05:06,707 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2016-12-22 14:05:06,707 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2016-12-22 14:05:06,711 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 10001)
2016-12-22 14:05:06,711 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
2016-12-22 14:05:06,713 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
Model performance stopped improving. Decreasing learning rate from 0.5 to 0.25 and resetting state to 100 % of epoch 3.
Finished training epoch 3 in 0 hours 4.6 minutes. Best validation perplexity 124.05.
Training finished in 0 hours 23.1 minutes.
2016-12-22 14:05:06,715 set_state: layers/projection_layer/W <- array(10001, 100)
2016-12-22 14:05:06,716 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-12-22 14:05:06,716 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-12-22 14:05:06,717 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-12-22 14:05:06,717 set_state: layers/output_layer/input/b <- array(10001,)
2016-12-22 14:05:06,720 set_state: layers/output_layer/input/W <- array(256, 10001)
Best validation set perplexity: 124.161278084
train finished.
Computing evaluation set perplexity.
Reading vocabulary from network state.
Number of words in vocabulary: 10001
Number of word classes: 10001
Building neural network.
Restoring neural network state.
Building text scorer.
Number of sentences: 3761
Number of words: 86191
Number of out-of-vocabulary words: 4794
Number of predicted probabilities: 82430
Cross entropy (base e): 4.776156171840011
Perplexity: 118.64741214811696
