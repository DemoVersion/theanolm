THEANO_FLAGS=floatX=float32,device=cuda0,contexts=dev0->cuda0;dev1->cuda0,nvcc.fastmath=True
/l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab
Reading vocabulary from /l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab.
Number of words in vocabulary: 10001
Number of word classes: 10001
2016-12-07 23:38:47,796 train: TRAINING OPTIONS
2016-12-07 23:38:47,796 train: batch_size: 32
2016-12-07 23:38:47,796 train: min_epochs: 1
2016-12-07 23:38:47,796 train: max_epochs: 15
2016-12-07 23:38:47,796 train: validation_frequency: 1
2016-12-07 23:38:47,796 train: sequence_length: 25
2016-12-07 23:38:47,796 train: stopping_criterion: no-improvement
2016-12-07 23:38:47,796 train: max_annealing_count: 0
2016-12-07 23:38:47,796 train: patience: 0
2016-12-07 23:38:47,796 train: OPTIMIZATION OPTIONS
2016-12-07 23:38:47,797 train: cost_function: cross-entropy
2016-12-07 23:38:47,797 train: max_gradient_norm: 5.0
2016-12-07 23:38:47,797 train: gradient_decay_rate: 0.9
2016-12-07 23:38:47,797 train: sqr_gradient_decay_rate: 0.999
2016-12-07 23:38:47,797 train: epsilon: 1e-06
2016-12-07 23:38:47,797 train: learning_rate: 1.0
2016-12-07 23:38:47,797 train: num_noise_samples: 1
2016-12-07 23:38:47,797 train: momentum: 0.9
2016-12-07 23:38:47,797 train: ignore_unk: False
2016-12-07 23:38:47,797 train: method: adagrad
2016-12-07 23:38:47,797 train: noise_sharing: None
2016-12-07 23:38:47,797 train: unk_penalty: None
2016-12-07 23:38:47,797 train: weights: [ 1.]
Creating trainer.
Computing unigram probabilities and the number of mini-batches in training data.
2016-12-07 23:38:48,899 __init__: One epoch of training data contains 1778 mini-batch updates.
2016-12-07 23:38:48,899 __init__: Class unigram probabilities are in the range [0.00000103, 0.05232915].
2016-12-07 23:38:48,900 __init__: Finding sentence start positions in /teamwork/t40511_asr/c/penn-treebank-project/ptb.train.txt.
2016-12-07 23:38:48,922 _reset: Generating a random order of input lines.
Building neural network.
2016-12-07 23:38:48,953 __init__: Creating layers.
2016-12-07 23:38:48,953 __init__: - NetworkInput name=word_input inputs=[] size=10001, devices=[]
2016-12-07 23:38:48,954 __init__: - ProjectionLayer name=projection_layer inputs=[word_input] size=100, devices=[dev0, dev1]
2016-12-07 23:38:49,000 add:      * layers/projection_layer/W/dev0 size=500050 type=float32 device=dev0
2016-12-07 23:38:49,002 add:      * layers/projection_layer/W/dev1 size=500050 type=float32 device=dev1
2016-12-07 23:38:49,002 __init__: - LSTMLayer name=hidden_layer inputs=[projection_layer] size=256, devices=[None]
2016-12-07 23:38:49,010 add:      * layers/hidden_layer/layer_input/W size=102400 type=float32 device=None
2016-12-07 23:38:49,253 add:      * layers/hidden_layer/step_input/W size=262144 type=float32 device=None
2016-12-07 23:38:49,253 add:      * layers/hidden_layer/layer_input/b size=1024 type=float32 device=None
2016-12-07 23:38:49,253 __init__: - SoftmaxLayer name=output_layer inputs=[hidden_layer] size=10001, devices=[None]
2016-12-07 23:38:49,472 add:      * layers/output_layer/input/W size=2560256 type=float32 device=None
2016-12-07 23:38:49,472 add:      * layers/output_layer/input/b size=10001 type=float32 device=None
2016-12-07 23:38:49,472 __init__: Total number of parameters: 3935925
Compiling optimization function.
2016-12-07 23:38:52,410 add:      * layers/hidden_layer/step_input/W_gradient size=262144 type=float32 device=None
2016-12-07 23:38:52,411 add:      * layers/hidden_layer/step_input/W_sum_sqr_gradient size=262144 type=float32 device=None
2016-12-07 23:38:52,411 add:      * layers/output_layer/input/b_gradient size=10001 type=float32 device=None
2016-12-07 23:38:52,411 add:      * layers/output_layer/input/b_sum_sqr_gradient size=10001 type=float32 device=None
2016-12-07 23:38:52,412 add:      * layers/projection_layer/W/dev1_gradient size=500050 type=float32 device=None
2016-12-07 23:38:52,413 add:      * layers/projection_layer/W/dev1_sum_sqr_gradient size=500050 type=float32 device=None
2016-12-07 23:38:52,414 add:      * layers/projection_layer/W/dev0_gradient size=500050 type=float32 device=None
2016-12-07 23:38:52,415 add:      * layers/projection_layer/W/dev0_sum_sqr_gradient size=500050 type=float32 device=None
2016-12-07 23:38:52,416 add:      * layers/hidden_layer/layer_input/W_gradient size=102400 type=float32 device=None
2016-12-07 23:38:52,416 add:      * layers/hidden_layer/layer_input/W_sum_sqr_gradient size=102400 type=float32 device=None
2016-12-07 23:38:52,421 add:      * layers/output_layer/input/W_gradient size=2560256 type=float32 device=None
2016-12-07 23:38:52,425 add:      * layers/output_layer/input/W_sum_sqr_gradient size=2560256 type=float32 device=None
2016-12-07 23:38:52,426 add:      * layers/hidden_layer/layer_input/b_gradient size=1024 type=float32 device=None
2016-12-07 23:38:52,426 add:      * layers/hidden_layer/layer_input/b_sum_sqr_gradient size=1024 type=float32 device=None
2016-12-07 23:38:55,810 warn: SeqOptimizer apply <theano.gpuarray.opt.GraphToGPU object at 0x7f9ee198b080>
2016-12-07 23:38:55,810 warn: Traceback:
2016-12-07 23:38:55,825 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 235, in apply
    sub_prof = optimizer.optimize(fgraph)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 87, in optimize
    ret = self.apply(fgraph, *args, **kwargs)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/opt.py", line 370, in apply
    node.outputs)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2911, in local_gpua_softmax_dnn_grad
    raise_no_cudnn("cuDNN needed for SoftmaxGrad")
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN needed for SoftmaxGrad

2016-12-07 23:38:55,954 warn: Optimization failure due to: local_gpua_softmax_dnn_grad
2016-12-07 23:38:55,954 warn: node: SoftmaxGrad(AdvancedIncSubtensor{inplace=False,  set_instead_of_inc=False}.0, Softmax.0)
2016-12-07 23:38:55,954 warn: TRACEBACK:
2016-12-07 23:38:55,954 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/opt.py", line 217, in local_opt
    new_op = maker(node.op, context_name, node.inputs, node.outputs)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2911, in local_gpua_softmax_dnn_grad
    raise_no_cudnn("cuDNN needed for SoftmaxGrad")
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN needed for SoftmaxGrad

2016-12-07 23:38:56,039 warn: Optimization failure due to: local_gpua_softmax_dnn_grad
2016-12-07 23:38:56,039 warn: node: SoftmaxGrad(AdvancedIncSubtensor{inplace=False,  set_instead_of_inc=False}.0, Softmax.0)
2016-12-07 23:38:56,039 warn: TRACEBACK:
2016-12-07 23:38:56,040 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/opt.py", line 217, in local_opt
    new_op = maker(node.op, context_name, node.inputs, node.outputs)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2911, in local_gpua_softmax_dnn_grad
    raise_no_cudnn("cuDNN needed for SoftmaxGrad")
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN needed for SoftmaxGrad

2016-12-07 23:38:56,079 warn: Optimization failure due to: local_gpua_softmax_dnn_grad
2016-12-07 23:38:56,079 warn: node: SoftmaxGrad(AdvancedIncSubtensor{inplace=False,  set_instead_of_inc=False}.0, Softmax.0)
2016-12-07 23:38:56,079 warn: TRACEBACK:
2016-12-07 23:38:56,079 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/opt.py", line 217, in local_opt
    new_op = maker(node.op, context_name, node.inputs, node.outputs)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2911, in local_gpua_softmax_dnn_grad
    raise_no_cudnn("cuDNN needed for SoftmaxGrad")
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN needed for SoftmaxGrad

2016-12-07 23:38:56,101 warn: Optimization failure due to: local_gpua_softmax_dnn_grad
2016-12-07 23:38:56,101 warn: node: SoftmaxGrad(AdvancedIncSubtensor{inplace=False,  set_instead_of_inc=False}.0, Softmax.0)
2016-12-07 23:38:56,101 warn: TRACEBACK:
2016-12-07 23:38:56,101 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/opt.py", line 217, in local_opt
    new_op = maker(node.op, context_name, node.inputs, node.outputs)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2911, in local_gpua_softmax_dnn_grad
    raise_no_cudnn("cuDNN needed for SoftmaxGrad")
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN needed for SoftmaxGrad

2016-12-07 23:38:56,121 warn: Optimization failure due to: local_gpua_softmax_dnn_grad
2016-12-07 23:38:56,121 warn: node: SoftmaxGrad(AdvancedIncSubtensor{inplace=False,  set_instead_of_inc=False}.0, HostFromGpu(gpuarray).0)
2016-12-07 23:38:56,121 warn: TRACEBACK:
2016-12-07 23:38:56,121 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/opt.py", line 217, in local_opt
    new_op = maker(node.op, context_name, node.inputs, node.outputs)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2911, in local_gpua_softmax_dnn_grad
    raise_no_cudnn("cuDNN needed for SoftmaxGrad")
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN needed for SoftmaxGrad

2016-12-07 23:38:56,123 warn: Optimization failure due to: local_softmax_dnn
2016-12-07 23:38:56,124 warn: node: GpuSoftmax(GpuReshape{2}.0)
2016-12-07 23:38:56,124 warn: TRACEBACK:
2016-12-07 23:38:56,124 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2842, in local_softmax_dnn
    raise_no_cudnn()
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN is required for convolution and pooling

2016-12-07 23:38:56,140 warn: Optimization failure due to: local_gpua_softmax_dnn_grad
2016-12-07 23:38:56,140 warn: node: SoftmaxGrad(AdvancedIncSubtensor{inplace=False,  set_instead_of_inc=False}.0, HostFromGpu(gpuarray).0)
2016-12-07 23:38:56,140 warn: TRACEBACK:
2016-12-07 23:38:56,140 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/opt.py", line 217, in local_opt
    new_op = maker(node.op, context_name, node.inputs, node.outputs)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2911, in local_gpua_softmax_dnn_grad
    raise_no_cudnn("cuDNN needed for SoftmaxGrad")
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN needed for SoftmaxGrad

2016-12-07 23:38:56,141 warn: Optimization failure due to: local_softmax_dnn
2016-12-07 23:38:56,141 warn: node: GpuSoftmax(GpuReshape{2}.0)
2016-12-07 23:38:56,141 warn: TRACEBACK:
2016-12-07 23:38:56,141 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2842, in local_softmax_dnn
    raise_no_cudnn()
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN is required for convolution and pooling

Building text scorer for cross-validation.
2016-12-07 23:39:43,590 warn: Optimization failure due to: local_softmax_dnn
2016-12-07 23:39:43,590 warn: node: GpuSoftmax(GpuReshape{2}.0)
2016-12-07 23:39:43,590 warn: TRACEBACK:
2016-12-07 23:39:43,596 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2842, in local_softmax_dnn
    raise_no_cudnn()
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN is required for convolution and pooling

2016-12-07 23:39:43,600 warn: Optimization failure due to: local_softmax_dnn
2016-12-07 23:39:43,600 warn: node: GpuSoftmax(GpuReshape{2}.0)
2016-12-07 23:39:43,600 warn: TRACEBACK:
2016-12-07 23:39:43,600 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2842, in local_softmax_dnn
    raise_no_cudnn()
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN is required for convolution and pooling

2016-12-07 23:39:43,604 warn: Optimization failure due to: local_softmax_dnn
2016-12-07 23:39:43,605 warn: node: GpuSoftmax(GpuReshape{2}.0)
2016-12-07 23:39:43,605 warn: TRACEBACK:
2016-12-07 23:39:43,605 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2842, in local_softmax_dnn
    raise_no_cudnn()
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN is required for convolution and pooling

2016-12-07 23:39:47,988 warn: Optimization failure due to: local_softmax_dnn
2016-12-07 23:39:47,988 warn: node: GpuSoftmax(GpuReshape{2}.0)
2016-12-07 23:39:47,988 warn: TRACEBACK:
2016-12-07 23:39:47,992 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2842, in local_softmax_dnn
    raise_no_cudnn()
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN is required for convolution and pooling

2016-12-07 23:39:47,997 warn: Optimization failure due to: local_softmax_dnn
2016-12-07 23:39:47,997 warn: node: GpuSoftmax(GpuReshape{2}.0)
2016-12-07 23:39:47,997 warn: TRACEBACK:
2016-12-07 23:39:47,997 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2842, in local_softmax_dnn
    raise_no_cudnn()
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN is required for convolution and pooling

2016-12-07 23:39:48,001 warn: Optimization failure due to: local_softmax_dnn
2016-12-07 23:39:48,001 warn: node: GpuSoftmax(GpuReshape{2}.0)
2016-12-07 23:39:48,001 warn: TRACEBACK:
2016-12-07 23:39:48,001 warn: Traceback (most recent call last):
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gof/opt.py", line 1922, in process_node
    replacements = lopt.transform(node)
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 2842, in local_softmax_dnn
    raise_no_cudnn()
  File "/teamwork/t40511_asr/Modules/opt/Theano/Theano-c927092/lib/python3.5/site-packages/Theano-0.9.0.dev4-py3.5.egg/theano/gpuarray/dnn.py", line 89, in raise_no_cudnn
    raise RuntimeError(msg)
RuntimeError: cuDNN is required for convolution and pooling

Validation text: /teamwork/t40511_asr/c/penn-treebank-project/ptb.valid.txt
Training neural network.
2016-12-07 23:40:19,282 _log_update: [200] (11.2 %) of epoch 1 -- lr = 1, cost = 5.76, duration = 12.8 ms
2016-12-07 23:40:45,120 _log_update: [400] (22.5 %) of epoch 1 -- lr = 1, cost = 5.69, duration = 12.8 ms
2016-12-07 23:41:10,976 _log_update: [600] (33.7 %) of epoch 1 -- lr = 1, cost = 5.14, duration = 12.8 ms
2016-12-07 23:41:36,866 _log_update: [800] (45.0 %) of epoch 1 -- lr = 1, cost = 5.20, duration = 12.8 ms
2016-12-07 23:42:02,718 _log_update: [1000] (56.2 %) of epoch 1 -- lr = 1, cost = 5.39, duration = 12.8 ms
2016-12-07 23:42:28,576 _log_update: [1200] (67.5 %) of epoch 1 -- lr = 1, cost = 5.12, duration = 12.8 ms
2016-12-07 23:42:54,431 _log_update: [1400] (78.7 %) of epoch 1 -- lr = 1, cost = 5.43, duration = 12.8 ms
2016-12-07 23:43:20,304 _log_update: [1600] (90.0 %) of epoch 1 -- lr = 1, cost = 4.92, duration = 12.8 ms
2016-12-07 23:43:47,629 _validate: [1772] First validation sample, perplexity 147.58.
2016-12-07 23:44:03,152 _validate: [1775] Center of validation, perplexity 147.59.
2016-12-07 23:44:18,795 _validate: [1778] Last validation sample, perplexity 147.64.
2016-12-07 23:44:18,830 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-12-07 23:44:18,830 _log_validation: [1778] Validation set cost history: [147.6]
2016-12-07 23:44:18,830 _reset: Generating a random order of input lines.
Finished training epoch 1 in 0 hours 4.5 minutes. Best validation perplexity 147.60.
2016-12-07 23:44:21,644 _log_update: [22] (1.2 %) of epoch 2 -- lr = 1, cost = 4.45, duration = 12.9 ms
2016-12-07 23:44:47,519 _log_update: [222] (12.5 %) of epoch 2 -- lr = 1, cost = 4.40, duration = 12.8 ms
2016-12-07 23:45:13,621 _log_update: [422] (23.7 %) of epoch 2 -- lr = 1, cost = 4.83, duration = 12.9 ms
2016-12-07 23:45:39,655 _log_update: [622] (35.0 %) of epoch 2 -- lr = 1, cost = 4.73, duration = 13.5 ms
2016-12-07 23:46:05,529 _log_update: [822] (46.2 %) of epoch 2 -- lr = 1, cost = 4.81, duration = 12.9 ms
2016-12-07 23:46:31,389 _log_update: [1022] (57.5 %) of epoch 2 -- lr = 1, cost = 4.72, duration = 12.8 ms
2016-12-07 23:46:57,242 _log_update: [1222] (68.7 %) of epoch 2 -- lr = 1, cost = 4.60, duration = 12.8 ms
2016-12-07 23:47:23,098 _log_update: [1422] (80.0 %) of epoch 2 -- lr = 1, cost = 4.94, duration = 12.8 ms
2016-12-07 23:47:48,971 _log_update: [1622] (91.2 %) of epoch 2 -- lr = 1, cost = 4.52, duration = 12.8 ms
2016-12-07 23:48:13,451 _validate: [1772] First validation sample, perplexity 126.46.
2016-12-07 23:48:28,978 _validate: [1775] Center of validation, perplexity 126.56.
2016-12-07 23:48:44,535 _validate: [1778] Last validation sample, perplexity 126.85.
2016-12-07 23:48:44,568 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-12-07 23:48:44,568 _log_validation: [1778] Validation set cost history: 147.6 [126.6]
2016-12-07 23:48:44,570 _reset: Generating a random order of input lines.
Finished training epoch 2 in 0 hours 4.4 minutes. Best validation perplexity 126.56.
2016-12-07 23:48:50,239 _log_update: [44] (2.5 %) of epoch 3 -- lr = 1, cost = 4.45, duration = 12.8 ms
2016-12-07 23:49:16,098 _log_update: [244] (13.7 %) of epoch 3 -- lr = 1, cost = 4.08, duration = 12.9 ms
2016-12-07 23:49:41,967 _log_update: [444] (25.0 %) of epoch 3 -- lr = 1, cost = 4.02, duration = 12.8 ms
2016-12-07 23:50:07,837 _log_update: [644] (36.2 %) of epoch 3 -- lr = 1, cost = 4.35, duration = 12.9 ms
2016-12-07 23:50:33,692 _log_update: [844] (47.5 %) of epoch 3 -- lr = 1, cost = 4.58, duration = 12.8 ms
2016-12-07 23:50:59,553 _log_update: [1044] (58.7 %) of epoch 3 -- lr = 1, cost = 4.61, duration = 12.9 ms
2016-12-07 23:51:25,416 _log_update: [1244] (70.0 %) of epoch 3 -- lr = 1, cost = 4.52, duration = 12.8 ms
2016-12-07 23:51:51,269 _log_update: [1444] (81.2 %) of epoch 3 -- lr = 1, cost = 4.64, duration = 12.8 ms
2016-12-07 23:52:17,125 _log_update: [1644] (92.5 %) of epoch 3 -- lr = 1, cost = 4.52, duration = 12.8 ms
2016-12-07 23:52:38,762 _validate: [1772] First validation sample, perplexity 124.72.
2016-12-07 23:52:54,285 _validate: [1775] Center of validation, perplexity 124.37.
2016-12-07 23:53:09,840 _validate: [1778] Last validation sample, perplexity 124.38.
2016-12-07 23:53:09,862 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2016-12-07 23:53:09,862 _log_validation: [1778] Validation set cost history: 147.6 126.6 [124.4]
2016-12-07 23:53:09,864 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 4.4 minutes. Best validation perplexity 124.38.
2016-12-07 23:53:18,366 _log_update: [66] (3.7 %) of epoch 4 -- lr = 1, cost = 4.13, duration = 12.8 ms
2016-12-07 23:53:44,227 _log_update: [266] (15.0 %) of epoch 4 -- lr = 1, cost = 4.22, duration = 12.8 ms
2016-12-07 23:54:10,079 _log_update: [466] (26.2 %) of epoch 4 -- lr = 1, cost = 4.24, duration = 12.9 ms
2016-12-07 23:54:35,930 _log_update: [666] (37.5 %) of epoch 4 -- lr = 1, cost = 3.86, duration = 12.8 ms
2016-12-07 23:55:01,806 _log_update: [866] (48.7 %) of epoch 4 -- lr = 1, cost = 3.99, duration = 12.8 ms
2016-12-07 23:55:27,681 _log_update: [1066] (60.0 %) of epoch 4 -- lr = 1, cost = 3.80, duration = 12.8 ms
2016-12-07 23:55:53,534 _log_update: [1266] (71.2 %) of epoch 4 -- lr = 1, cost = 4.48, duration = 12.8 ms
2016-12-07 23:56:19,393 _log_update: [1466] (82.5 %) of epoch 4 -- lr = 1, cost = 4.33, duration = 12.8 ms
2016-12-07 23:56:45,243 _log_update: [1666] (93.7 %) of epoch 4 -- lr = 1, cost = 4.33, duration = 12.8 ms
2016-12-07 23:57:04,033 _validate: [1772] First validation sample, perplexity 128.58.
2016-12-07 23:57:19,556 _validate: [1775] Center of validation, perplexity 128.53.
2016-12-07 23:57:35,118 _validate: [1778] Last validation sample, perplexity 128.13.
2016-12-07 23:57:35,118 _log_validation: [1778] Validation set cost history: 147.6 126.6 [124.4] 128.5
2016-12-07 23:57:35,119 set_state: layers/projection_layer/W/dev1 <- array(10001, 50)
2016-12-07 23:57:35,120 set_state: layers/projection_layer/W/dev0 <- array(10001, 50)
2016-12-07 23:57:35,121 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-12-07 23:57:35,122 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-12-07 23:57:35,122 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-12-07 23:57:35,126 set_state: layers/output_layer/input/W <- array(256, 10001)
2016-12-07 23:57:35,126 set_state: layers/output_layer/input/b <- array(10001,)
2016-12-07 23:57:35,128 _reset_state: [1775] (99.83 %) of epoch 3
2016-12-07 23:57:35,128 _log_validation: [1775] Validation set cost history: 147.6 126.6 [124.4]
2016-12-07 23:57:35,128 set_state: Restored iterator to line 42000 of 42068.
2016-12-07 23:57:35,129 set_state: layers/projection_layer/W/dev0_gradient <- array(10001, 50)
2016-12-07 23:57:35,130 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2016-12-07 23:57:35,130 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2016-12-07 23:57:35,131 set_state: layers/output_layer/input/b_gradient <- array(10001,)
2016-12-07 23:57:35,131 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
2016-12-07 23:57:35,132 set_state: layers/projection_layer/W/dev1_sum_sqr_gradient <- array(10001, 50)
2016-12-07 23:57:35,132 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2016-12-07 23:57:35,133 set_state: layers/projection_layer/W/dev1_gradient <- array(10001, 50)
2016-12-07 23:57:35,133 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2016-12-07 23:57:35,136 set_state: layers/output_layer/input/W_gradient <- array(256, 10001)
2016-12-07 23:57:35,141 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 10001)
2016-12-07 23:57:35,142 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(10001,)
2016-12-07 23:57:35,142 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2016-12-07 23:57:35,143 set_state: layers/projection_layer/W/dev0_sum_sqr_gradient <- array(10001, 50)
Model performance stopped improving. Decreasing learning rate from 1.0 to 0.5 and resetting state to 100 % of epoch 3.
2016-12-07 23:57:35,144 _reset: Generating a random order of input lines.
Finished training epoch 3 in 0 hours 4.4 minutes. Best validation perplexity 124.38.
2016-12-07 23:57:46,111 _log_update: [88] (4.9 %) of epoch 4 -- lr = 0.5, cost = 3.89, duration = 12.4 ms
2016-12-07 23:58:11,080 _log_update: [288] (16.2 %) of epoch 4 -- lr = 0.5, cost = 4.11, duration = 12.4 ms
2016-12-07 23:58:36,071 _log_update: [488] (27.4 %) of epoch 4 -- lr = 0.5, cost = 4.09, duration = 12.4 ms
2016-12-07 23:59:01,065 _log_update: [688] (38.7 %) of epoch 4 -- lr = 0.5, cost = 3.96, duration = 12.4 ms
2016-12-07 23:59:26,044 _log_update: [888] (49.9 %) of epoch 4 -- lr = 0.5, cost = 4.07, duration = 12.4 ms
2016-12-07 23:59:51,011 _log_update: [1088] (61.2 %) of epoch 4 -- lr = 0.5, cost = 3.95, duration = 12.4 ms
2016-12-08 00:00:15,993 _log_update: [1288] (72.4 %) of epoch 4 -- lr = 0.5, cost = 4.04, duration = 12.4 ms
2016-12-08 00:00:40,967 _log_update: [1488] (83.7 %) of epoch 4 -- lr = 0.5, cost = 4.10, duration = 12.4 ms
2016-12-08 00:01:05,939 _log_update: [1688] (94.9 %) of epoch 4 -- lr = 0.5, cost = 4.23, duration = 12.4 ms
2016-12-08 00:01:21,500 _validate: [1772] First validation sample, perplexity 125.58.
2016-12-08 00:01:37,002 _validate: [1775] Center of validation, perplexity 125.58.
2016-12-08 00:01:52,536 _validate: [1778] Last validation sample, perplexity 125.44.
2016-12-08 00:01:52,536 _log_validation: [1778] Validation set cost history: 147.6 126.6 [124.4] 125.6
2016-12-08 00:01:52,538 set_state: layers/projection_layer/W/dev1 <- array(10001, 50)
2016-12-08 00:01:52,539 set_state: layers/projection_layer/W/dev0 <- array(10001, 50)
2016-12-08 00:01:52,540 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-12-08 00:01:52,540 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-12-08 00:01:52,540 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-12-08 00:01:52,544 set_state: layers/output_layer/input/W <- array(256, 10001)
2016-12-08 00:01:52,545 set_state: layers/output_layer/input/b <- array(10001,)
2016-12-08 00:01:52,546 _reset_state: [1775] (99.83 %) of epoch 3
2016-12-08 00:01:52,546 _log_validation: [1775] Validation set cost history: 147.6 126.6 [124.4]
2016-12-08 00:01:52,547 set_state: Restored iterator to line 42000 of 42068.
2016-12-08 00:01:52,548 set_state: layers/projection_layer/W/dev0_gradient <- array(10001, 50)
2016-12-08 00:01:52,548 set_state: layers/hidden_layer/step_input/W_sum_sqr_gradient <- array(256, 1024)
2016-12-08 00:01:52,549 set_state: layers/hidden_layer/step_input/W_gradient <- array(256, 1024)
2016-12-08 00:01:52,549 set_state: layers/output_layer/input/b_gradient <- array(10001,)
2016-12-08 00:01:52,550 set_state: layers/hidden_layer/layer_input/W_gradient <- array(100, 1024)
2016-12-08 00:01:52,550 set_state: layers/projection_layer/W/dev1_sum_sqr_gradient <- array(10001, 50)
2016-12-08 00:01:52,551 set_state: layers/hidden_layer/layer_input/b_gradient <- array(1024,)
2016-12-08 00:01:52,551 set_state: layers/projection_layer/W/dev1_gradient <- array(10001, 50)
2016-12-08 00:01:52,552 set_state: layers/hidden_layer/layer_input/W_sum_sqr_gradient <- array(100, 1024)
2016-12-08 00:01:52,555 set_state: layers/output_layer/input/W_gradient <- array(256, 10001)
2016-12-08 00:01:52,559 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 10001)
2016-12-08 00:01:52,560 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(10001,)
2016-12-08 00:01:52,560 set_state: layers/hidden_layer/layer_input/b_sum_sqr_gradient <- array(1024,)
2016-12-08 00:01:52,561 set_state: layers/projection_layer/W/dev0_sum_sqr_gradient <- array(10001, 50)
Model performance stopped improving. Decreasing learning rate from 0.5 to 0.25 and resetting state to 100 % of epoch 3.
Finished training epoch 3 in 0 hours 4.3 minutes. Best validation perplexity 124.38.
Training finished in 0 hours 22.0 minutes.
2016-12-08 00:01:52,563 set_state: layers/projection_layer/W/dev1 <- array(10001, 50)
2016-12-08 00:01:52,564 set_state: layers/projection_layer/W/dev0 <- array(10001, 50)
2016-12-08 00:01:52,565 set_state: layers/hidden_layer/step_input/W <- array(256, 1024)
2016-12-08 00:01:52,565 set_state: layers/hidden_layer/layer_input/W <- array(100, 1024)
2016-12-08 00:01:52,565 set_state: layers/hidden_layer/layer_input/b <- array(1024,)
2016-12-08 00:01:52,569 set_state: layers/output_layer/input/W <- array(256, 10001)
2016-12-08 00:01:52,569 set_state: layers/output_layer/input/b <- array(10001,)
Best validation set perplexity: 124.37011352
train finished.
Computing evaluation set perplexity.
Reading vocabulary from network state.
Number of words in vocabulary: 10001
Number of word classes: 10001
Building neural network.
RuntimeError: Unable to create Theano shared variable for parameter layers/projection_layer/W/dev0 on device dev0. If you are using the old backend, you cannot assign layers to different GPU devices.
