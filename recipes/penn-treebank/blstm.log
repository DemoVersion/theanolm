/l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab
THEANO_FLAGS=floatX=float32,device=gpu,nvcc.fastmath=True
Reading vocabulary from /l/senarvi/theanolm-recipes/penn-treebank/nnlm.vocab.
Number of words in vocabulary: 10001
Number of word classes: 10001
2017-04-03 22:24:46,294 train: TRAINING OPTIONS
2017-04-03 22:24:46,294 train: patience: 0
2017-04-03 22:24:46,294 train: min_epochs: 1
2017-04-03 22:24:46,294 train: max_annealing_count: 0
2017-04-03 22:24:46,294 train: sequence_length: 25
2017-04-03 22:24:46,294 train: validation_frequency: 1
2017-04-03 22:24:46,294 train: batch_size: 32
2017-04-03 22:24:46,294 train: stopping_criterion: no-improvement
2017-04-03 22:24:46,294 train: max_epochs: 15
2017-04-03 22:24:46,294 train: OPTIMIZATION OPTIONS
2017-04-03 22:24:46,295 train: epsilon: 1e-06
2017-04-03 22:24:46,295 train: noise_sharing: None
2017-04-03 22:24:46,295 train: unk_penalty: None
2017-04-03 22:24:46,295 train: learning_rate: 1.0
2017-04-03 22:24:46,295 train: num_noise_samples: 1
2017-04-03 22:24:46,295 train: cost_function: cross-entropy
2017-04-03 22:24:46,295 train: weights: [ 1.]
2017-04-03 22:24:46,295 train: sqr_gradient_decay_rate: 0.999
2017-04-03 22:24:46,295 train: method: adagrad
2017-04-03 22:24:46,295 train: momentum: 0.9
2017-04-03 22:24:46,295 train: ignore_unk: False
2017-04-03 22:24:46,295 train: gradient_decay_rate: 0.9
2017-04-03 22:24:46,295 train: max_gradient_norm: 5.0
Creating trainer.
Computing unigram probabilities and the number of mini-batches in training data.
2017-04-03 22:24:47,412 __init__: One epoch of training data contains 1778 mini-batch updates.
2017-04-03 22:24:47,413 __init__: Class unigram probabilities are in the range [0.00000103, 0.05232915].
2017-04-03 22:24:47,413 __init__: Finding sentence start positions in /teamwork/t40511_asr/c/penn-treebank-project/ptb.train.txt.
2017-04-03 22:24:47,436 _reset: Generating a random order of input lines.
Building neural network.
2017-04-03 22:24:47,454 __init__: Creating layers.
2017-04-03 22:24:47,454 __init__: - NetworkInput name=word_input inputs=[] size=10001, devices=[]
2017-04-03 22:24:47,455 __init__: - ProjectionLayer name=projection_layer inputs=[word_input] size=100, devices=[None]
2017-04-03 22:24:47,537 add:      * layers/projection_layer/W size=1000100 type=float32 device=None
2017-04-03 22:24:47,537 __init__: - LSTMLayer name=hidden_layer/forward inputs=[projection_layer] size=128, devices=[None]
2017-04-03 22:24:47,541 add:      * layers/hidden_layer/forward/layer_input/W size=51200 type=float32 device=None
2017-04-03 22:24:47,633 add:      * layers/hidden_layer/forward/step_input/W size=65536 type=float32 device=None
2017-04-03 22:24:47,633 add:      * layers/hidden_layer/forward/layer_input/b size=512 type=float32 device=None
2017-04-03 22:24:47,633 __init__: - LSTMLayer name=hidden_layer/backward inputs=[projection_layer] size=128, devices=[None]
2017-04-03 22:24:47,638 add:      * layers/hidden_layer/backward/layer_input/W size=51200 type=float32 device=None
2017-04-03 22:24:47,692 add:      * layers/hidden_layer/backward/step_input/W size=65536 type=float32 device=None
2017-04-03 22:24:47,693 add:      * layers/hidden_layer/backward/layer_input/b size=512 type=float32 device=None
2017-04-03 22:24:47,693 __init__: - SoftmaxLayer name=output_layer inputs=[hidden_layer] size=10001, devices=[None]
2017-04-03 22:24:47,894 add:      * layers/output_layer/input/W size=2560256 type=float32 device=None
2017-04-03 22:24:47,894 add:      * layers/output_layer/input/b size=10001 type=float32 device=None
2017-04-03 22:24:47,894 __init__: Total number of parameters: 3804853
Compiling optimization function.
2017-04-03 22:24:51,331 add:      * layers/hidden_layer/backward/layer_input/W_gradient size=51200 type=float32 device=None
2017-04-03 22:24:51,331 add:      * layers/hidden_layer/backward/layer_input/W_sum_sqr_gradient size=51200 type=float32 device=None
2017-04-03 22:24:51,331 add:      * layers/hidden_layer/forward/layer_input/b_gradient size=512 type=float32 device=None
2017-04-03 22:24:51,332 add:      * layers/hidden_layer/forward/layer_input/b_sum_sqr_gradient size=512 type=float32 device=None
2017-04-03 22:24:51,332 add:      * layers/hidden_layer/forward/step_input/W_gradient size=65536 type=float32 device=None
2017-04-03 22:24:51,332 add:      * layers/hidden_layer/forward/step_input/W_sum_sqr_gradient size=65536 type=float32 device=None
2017-04-03 22:24:51,332 add:      * layers/output_layer/input/b_gradient size=10001 type=float32 device=None
2017-04-03 22:24:51,332 add:      * layers/output_layer/input/b_sum_sqr_gradient size=10001 type=float32 device=None
2017-04-03 22:24:51,332 add:      * layers/hidden_layer/forward/layer_input/W_gradient size=51200 type=float32 device=None
2017-04-03 22:24:51,333 add:      * layers/hidden_layer/forward/layer_input/W_sum_sqr_gradient size=51200 type=float32 device=None
2017-04-03 22:24:51,337 add:      * layers/output_layer/input/W_gradient size=2560256 type=float32 device=None
2017-04-03 22:24:51,340 add:      * layers/output_layer/input/W_sum_sqr_gradient size=2560256 type=float32 device=None
2017-04-03 22:24:51,342 add:      * layers/projection_layer/W_gradient size=1000100 type=float32 device=None
2017-04-03 22:24:51,344 add:      * layers/projection_layer/W_sum_sqr_gradient size=1000100 type=float32 device=None
2017-04-03 22:24:51,344 add:      * layers/hidden_layer/backward/step_input/W_gradient size=65536 type=float32 device=None
2017-04-03 22:24:51,344 add:      * layers/hidden_layer/backward/step_input/W_sum_sqr_gradient size=65536 type=float32 device=None
2017-04-03 22:24:51,344 add:      * layers/hidden_layer/backward/layer_input/b_gradient size=512 type=float32 device=None
2017-04-03 22:24:51,344 add:      * layers/hidden_layer/backward/layer_input/b_sum_sqr_gradient size=512 type=float32 device=None
Building text scorer for cross-validation.
Validation text: /teamwork/t40511_asr/c/penn-treebank-project/ptb.valid.txt
Training neural network.
2017-04-03 22:27:30,593 _log_update: [200] (11.2 %) of epoch 1 -- lr = 1, cost = 5.74, duration = 59.8 ms
2017-04-03 22:29:31,971 _log_update: [400] (22.5 %) of epoch 1 -- lr = 1, cost = 5.63, duration = 60.0 ms
2017-04-03 22:31:34,656 _log_update: [600] (33.7 %) of epoch 1 -- lr = 1, cost = 5.15, duration = 60.0 ms
2017-04-03 22:33:34,718 _log_update: [800] (45.0 %) of epoch 1 -- lr = 1, cost = 5.14, duration = 60.1 ms
2017-04-03 22:35:34,798 _log_update: [1000] (56.2 %) of epoch 1 -- lr = 1, cost = 5.28, duration = 59.8 ms
2017-04-03 22:37:35,173 _log_update: [1200] (67.5 %) of epoch 1 -- lr = 1, cost = 5.00, duration = 60.1 ms
2017-04-03 22:39:35,169 _log_update: [1400] (78.7 %) of epoch 1 -- lr = 1, cost = 5.21, duration = 60.0 ms
2017-04-03 22:41:35,365 _log_update: [1600] (90.0 %) of epoch 1 -- lr = 1, cost = 4.65, duration = 60.0 ms
2017-04-03 22:44:19,685 _validate: [1772] First validation sample, perplexity 177.21.
2017-04-03 22:47:24,835 _validate: [1775] Center of validation, perplexity 172.73.
2017-04-03 22:50:29,613 _validate: [1778] Last validation sample, perplexity 171.64.
2017-04-03 22:50:29,655 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2017-04-03 22:50:29,655 _log_validation: [1778] Validation set cost history: [177.2]
2017-04-03 22:50:29,656 _reset: Generating a random order of input lines.
Finished training epoch 1 in 0 hours 25.0 minutes. Best validation perplexity 177.21.
2017-04-03 22:50:42,856 _log_update: [22] (1.2 %) of epoch 2 -- lr = 1, cost = 4.46, duration = 59.5 ms
2017-04-03 22:52:42,927 _log_update: [222] (12.5 %) of epoch 2 -- lr = 1, cost = 4.32, duration = 59.7 ms
2017-04-03 22:54:42,731 _log_update: [422] (23.7 %) of epoch 2 -- lr = 1, cost = 4.32, duration = 59.4 ms
2017-04-03 22:56:43,252 _log_update: [622] (35.0 %) of epoch 2 -- lr = 1, cost = 4.35, duration = 59.6 ms
2017-04-03 22:58:42,743 _log_update: [822] (46.2 %) of epoch 2 -- lr = 1, cost = 4.43, duration = 59.6 ms
2017-04-03 23:00:42,326 _log_update: [1022] (57.5 %) of epoch 2 -- lr = 1, cost = 4.43, duration = 59.5 ms
2017-04-03 23:02:42,157 _log_update: [1222] (68.7 %) of epoch 2 -- lr = 1, cost = 4.74, duration = 64.6 ms
2017-04-03 23:04:41,892 _log_update: [1422] (80.0 %) of epoch 2 -- lr = 1, cost = 4.17, duration = 59.7 ms
2017-04-03 23:06:41,929 _log_update: [1622] (91.2 %) of epoch 2 -- lr = 1, cost = 3.69, duration = 59.9 ms
2017-04-03 23:09:12,965 _validate: [1772] First validation sample, perplexity 171.33.
2017-04-03 23:12:18,343 _validate: [1775] Center of validation, perplexity 158.23.
2017-04-03 23:15:23,296 _validate: [1778] Last validation sample, perplexity 161.78.
2017-04-03 23:15:23,319 _set_candidate_state: New candidate for optimal state saved to /l/senarvi/theanolm-recipes/penn-treebank/nnlm.h5.
2017-04-03 23:15:23,320 _log_validation: [1778] Validation set cost history: 177.2 [166.6]
2017-04-03 23:15:23,321 _reset: Generating a random order of input lines.
Finished training epoch 2 in 0 hours 24.9 minutes. Best validation perplexity 166.60.
2017-04-03 23:15:49,699 _log_update: [44] (2.5 %) of epoch 3 -- lr = 1, cost = 3.96, duration = 59.9 ms
2017-04-03 23:17:50,043 _log_update: [244] (13.7 %) of epoch 3 -- lr = 1, cost = 3.67, duration = 59.8 ms
2017-04-03 23:19:49,972 _log_update: [444] (25.0 %) of epoch 3 -- lr = 1, cost = 3.85, duration = 59.6 ms
2017-04-03 23:21:49,965 _log_update: [644] (36.2 %) of epoch 3 -- lr = 1, cost = 4.27, duration = 60.0 ms
2017-04-03 23:23:49,943 _log_update: [844] (47.5 %) of epoch 3 -- lr = 1, cost = 3.99, duration = 59.9 ms
2017-04-03 23:25:49,879 _log_update: [1044] (58.7 %) of epoch 3 -- lr = 1, cost = 3.63, duration = 60.0 ms
2017-04-03 23:27:50,069 _log_update: [1244] (70.0 %) of epoch 3 -- lr = 1, cost = 3.79, duration = 60.1 ms
2017-04-03 23:29:53,443 _log_update: [1444] (81.2 %) of epoch 3 -- lr = 1, cost = 4.05, duration = 59.9 ms
2017-04-03 23:31:53,583 _log_update: [1644] (92.5 %) of epoch 3 -- lr = 1, cost = 4.09, duration = 59.9 ms
2017-04-03 23:34:11,406 _validate: [1772] First validation sample, perplexity 175.23.
2017-04-03 23:37:16,704 _validate: [1775] Center of validation, perplexity 173.23.
2017-04-03 23:40:21,457 _validate: [1778] Last validation sample, perplexity 170.37.
2017-04-03 23:40:21,457 _log_validation: [1778] Validation set cost history: 177.2 [166.6] 173.4
2017-04-03 23:40:21,459 set_state: layers/projection_layer/W <- array(10001, 100)
2017-04-03 23:40:21,459 set_state: layers/hidden_layer/forward/layer_input/b <- array(512,)
2017-04-03 23:40:21,460 set_state: layers/hidden_layer/forward/step_input/W <- array(128, 512)
2017-04-03 23:40:21,460 set_state: layers/hidden_layer/forward/layer_input/W <- array(100, 512)
2017-04-03 23:40:21,460 set_state: layers/hidden_layer/backward/layer_input/W <- array(100, 512)
2017-04-03 23:40:21,461 set_state: layers/hidden_layer/backward/step_input/W <- array(128, 512)
2017-04-03 23:40:21,461 set_state: layers/hidden_layer/backward/layer_input/b <- array(512,)
2017-04-03 23:40:21,464 set_state: layers/output_layer/input/W <- array(256, 10001)
2017-04-03 23:40:21,464 set_state: layers/output_layer/input/b <- array(10001,)
2017-04-03 23:40:21,465 _reset_state: [1775] (99.83 %) of epoch 2
2017-04-03 23:40:21,466 _log_validation: [1775] Validation set cost history: 177.2 [166.6]
2017-04-03 23:40:21,466 set_state: Restored iterator to line 41997 of 42068.
2017-04-03 23:40:21,467 set_state: layers/hidden_layer/backward/layer_input/b_gradient <- array(512,)
2017-04-03 23:40:21,467 set_state: layers/hidden_layer/backward/step_input/W_sum_sqr_gradient <- array(128, 512)
2017-04-03 23:40:21,467 set_state: layers/hidden_layer/forward/step_input/W_gradient <- array(128, 512)
2017-04-03 23:40:21,468 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(10001,)
2017-04-03 23:40:21,468 set_state: layers/hidden_layer/forward/layer_input/b_gradient <- array(512,)
2017-04-03 23:40:21,469 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
2017-04-03 23:40:21,470 set_state: layers/output_layer/input/b_gradient <- array(10001,)
2017-04-03 23:40:21,470 set_state: layers/hidden_layer/forward/layer_input/W_sum_sqr_gradient <- array(100, 512)
2017-04-03 23:40:21,470 set_state: layers/hidden_layer/backward/layer_input/W_sum_sqr_gradient <- array(100, 512)
2017-04-03 23:40:21,473 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 10001)
2017-04-03 23:40:21,474 set_state: layers/hidden_layer/backward/layer_input/b_sum_sqr_gradient <- array(512,)
2017-04-03 23:40:21,476 set_state: layers/output_layer/input/W_gradient <- array(256, 10001)
2017-04-03 23:40:21,477 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2017-04-03 23:40:21,478 set_state: layers/hidden_layer/forward/layer_input/W_gradient <- array(100, 512)
2017-04-03 23:40:21,478 set_state: layers/hidden_layer/forward/step_input/W_sum_sqr_gradient <- array(128, 512)
2017-04-03 23:40:21,478 set_state: layers/hidden_layer/backward/step_input/W_gradient <- array(128, 512)
2017-04-03 23:40:21,479 set_state: layers/hidden_layer/backward/layer_input/W_gradient <- array(100, 512)
2017-04-03 23:40:21,479 set_state: layers/hidden_layer/forward/layer_input/b_sum_sqr_gradient <- array(512,)
Model performance stopped improving. Decreasing learning rate from 1.0 to 0.5 and resetting state to 100 % of epoch 2.
2017-04-03 23:40:21,481 _reset: Generating a random order of input lines.
Finished training epoch 2 in 0 hours 25.0 minutes. Best validation perplexity 166.60.
2017-04-03 23:41:01,009 _log_update: [66] (3.7 %) of epoch 3 -- lr = 0.5, cost = 3.90, duration = 59.5 ms
2017-04-03 23:43:00,523 _log_update: [266] (15.0 %) of epoch 3 -- lr = 0.5, cost = 4.08, duration = 59.8 ms
2017-04-03 23:45:00,093 _log_update: [466] (26.2 %) of epoch 3 -- lr = 0.5, cost = 3.92, duration = 59.6 ms
2017-04-03 23:47:00,068 _log_update: [666] (37.5 %) of epoch 3 -- lr = 0.5, cost = 3.73, duration = 59.7 ms
2017-04-03 23:48:59,601 _log_update: [866] (48.7 %) of epoch 3 -- lr = 0.5, cost = 4.23, duration = 59.5 ms
2017-04-03 23:50:59,067 _log_update: [1066] (60.0 %) of epoch 3 -- lr = 0.5, cost = 4.03, duration = 59.6 ms
2017-04-03 23:52:58,528 _log_update: [1266] (71.2 %) of epoch 3 -- lr = 0.5, cost = 3.69, duration = 59.5 ms
2017-04-03 23:54:58,632 _log_update: [1466] (82.5 %) of epoch 3 -- lr = 0.5, cost = 3.83, duration = 59.6 ms
2017-04-03 23:56:58,485 _log_update: [1666] (93.7 %) of epoch 3 -- lr = 0.5, cost = 3.83, duration = 59.6 ms
2017-04-03 23:59:02,942 _validate: [1772] First validation sample, perplexity 178.24.
2017-04-04 00:02:08,755 _validate: [1775] Center of validation, perplexity 174.36.
2017-04-04 00:05:13,614 _validate: [1778] Last validation sample, perplexity 171.15.
2017-04-04 00:05:13,615 _log_validation: [1778] Validation set cost history: 177.2 [166.6] 176.1
2017-04-04 00:05:13,616 set_state: layers/projection_layer/W <- array(10001, 100)
2017-04-04 00:05:13,617 set_state: layers/hidden_layer/forward/layer_input/b <- array(512,)
2017-04-04 00:05:13,617 set_state: layers/hidden_layer/forward/step_input/W <- array(128, 512)
2017-04-04 00:05:13,617 set_state: layers/hidden_layer/forward/layer_input/W <- array(100, 512)
2017-04-04 00:05:13,618 set_state: layers/hidden_layer/backward/layer_input/W <- array(100, 512)
2017-04-04 00:05:13,618 set_state: layers/hidden_layer/backward/step_input/W <- array(128, 512)
2017-04-04 00:05:13,618 set_state: layers/hidden_layer/backward/layer_input/b <- array(512,)
2017-04-04 00:05:13,621 set_state: layers/output_layer/input/W <- array(256, 10001)
2017-04-04 00:05:13,621 set_state: layers/output_layer/input/b <- array(10001,)
2017-04-04 00:05:13,623 _reset_state: [1775] (99.83 %) of epoch 2
2017-04-04 00:05:13,623 _log_validation: [1775] Validation set cost history: 177.2 [166.6]
2017-04-04 00:05:13,623 set_state: Restored iterator to line 41997 of 42068.
2017-04-04 00:05:13,624 set_state: layers/hidden_layer/backward/layer_input/b_gradient <- array(512,)
2017-04-04 00:05:13,624 set_state: layers/hidden_layer/backward/step_input/W_sum_sqr_gradient <- array(128, 512)
2017-04-04 00:05:13,624 set_state: layers/hidden_layer/forward/step_input/W_gradient <- array(128, 512)
2017-04-04 00:05:13,625 set_state: layers/output_layer/input/b_sum_sqr_gradient <- array(10001,)
2017-04-04 00:05:13,625 set_state: layers/hidden_layer/forward/layer_input/b_gradient <- array(512,)
2017-04-04 00:05:13,626 set_state: layers/projection_layer/W_sum_sqr_gradient <- array(10001, 100)
2017-04-04 00:05:13,627 set_state: layers/output_layer/input/b_gradient <- array(10001,)
2017-04-04 00:05:13,627 set_state: layers/hidden_layer/forward/layer_input/W_sum_sqr_gradient <- array(100, 512)
2017-04-04 00:05:13,627 set_state: layers/hidden_layer/backward/layer_input/W_sum_sqr_gradient <- array(100, 512)
2017-04-04 00:05:13,630 set_state: layers/output_layer/input/W_sum_sqr_gradient <- array(256, 10001)
2017-04-04 00:05:13,630 set_state: layers/hidden_layer/backward/layer_input/b_sum_sqr_gradient <- array(512,)
2017-04-04 00:05:13,633 set_state: layers/output_layer/input/W_gradient <- array(256, 10001)
2017-04-04 00:05:13,634 set_state: layers/projection_layer/W_gradient <- array(10001, 100)
2017-04-04 00:05:13,634 set_state: layers/hidden_layer/forward/layer_input/W_gradient <- array(100, 512)
2017-04-04 00:05:13,635 set_state: layers/hidden_layer/forward/step_input/W_sum_sqr_gradient <- array(128, 512)
2017-04-04 00:05:13,635 set_state: layers/hidden_layer/backward/step_input/W_gradient <- array(128, 512)
2017-04-04 00:05:13,636 set_state: layers/hidden_layer/backward/layer_input/W_gradient <- array(100, 512)
2017-04-04 00:05:13,636 set_state: layers/hidden_layer/forward/layer_input/b_sum_sqr_gradient <- array(512,)
Model performance stopped improving. Decreasing learning rate from 0.5 to 0.25 and resetting state to 100 % of epoch 2.
Finished training epoch 2 in 0 hours 24.9 minutes. Best validation perplexity 166.60.
Training finished in 1 hours 39.7 minutes.
2017-04-04 00:05:13,638 set_state: layers/projection_layer/W <- array(10001, 100)
2017-04-04 00:05:13,639 set_state: layers/hidden_layer/forward/layer_input/b <- array(512,)
2017-04-04 00:05:13,639 set_state: layers/hidden_layer/forward/step_input/W <- array(128, 512)
2017-04-04 00:05:13,640 set_state: layers/hidden_layer/forward/layer_input/W <- array(100, 512)
2017-04-04 00:05:13,640 set_state: layers/hidden_layer/backward/layer_input/W <- array(100, 512)
2017-04-04 00:05:13,640 set_state: layers/hidden_layer/backward/step_input/W <- array(128, 512)
2017-04-04 00:05:13,641 set_state: layers/hidden_layer/backward/layer_input/b <- array(512,)
2017-04-04 00:05:13,643 set_state: layers/output_layer/input/W <- array(256, 10001)
2017-04-04 00:05:13,644 set_state: layers/output_layer/input/b <- array(10001,)
Best validation set perplexity: 158.234405155
train finished.
Computing evaluation set perplexity.
Reading vocabulary from network state.
Number of words in vocabulary: 10001
Number of word classes: 10001
Building neural network.
Restoring neural network state.
Building text scorer.
Scoring text.
Number of sentences: 3761
Number of words: 86191
Number of tokens: 86191
Number of predicted probabilities: 82430
Number of excluded (OOV) words: 0
Cross entropy (base e): 4.949716201663014
Perplexity: 141.13490438617546
